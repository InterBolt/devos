#!/usr/bin/env bash
# shellcheck disable=SC2115
set -o errexit
set -o pipefail
set -o errtrace

cd "$(dirname "${BASH_SOURCE[0]}")"
vBIN_DIR="$(pwd)"
vBIN_FILEPATH="$vBIN_DIR/$0"
vDEBUG_LEVEL=${DEBUG_LEVEL:-0}
#
# These variables are shared between this script AND every helper script
# in the installer/bin directory.
#
# shellcheck source=solos.static
. "solos.static"
#
# `dotglob` option ensures that dotfiles and folders are included when using globs.
# Helpful for looping through files in a directory.
#
shopt -s dotglob
# --------------------------------------------------------------------------------------------
#
# RESPONSE/RETURN SLOTS
#
vPREV_CURL=""
vPREV_RETURN=()
# --------------------------------------------------------------------------------------------
#
# STATUSES:
#
# These are used to keep track of various statuses, like "did the bootstrap complete"
#
vSTATUS_BOOTSTRAPPED_POSTGRES="bootstrapped-remote-postgres"
vSTATUS_BOOTSTRAPPED_REMOTE="bootstrapped-remote"
vSTATUS_LAUNCH_SUCCEEDED="successful-run"
vSTATUS_BOOTSTRAPPED_DOCKER="bootstrapped-docker"
# --------------------------------------------------------------------------------------------
#
# SOURCE SCRIPTS
#
# Think of these like libraries.
# Except they are tightly coupled to the variables defined in this script.
#
# shellcheck source=shared.log.sh
. "shared.log.sh"
log.ready "cli"
#
# Now source all the things we need.
#
# shellcheck source=solos.cache
. "solos.cache"
# shellcheck source=solos.flags
. "solos.flags"
# shellcheck source=solos.ssh
. "solos.ssh"
# shellcheck source=solos.status
. "solos.status"
# shellcheck source=solos.utils
. "solos.utils"
# shellcheck source=solos.vultr
. "solos.vultr"
# shellcheck source=solos.precheck
. "solos.precheck"
# shellcheck source=solos.environment
. "solos.environment"
# shellcheck source=solos.test
. "solos.test"
#
# Return to the previous directory just in case.
#
cd "$vBIN_DIR"
#
# --------------------------------------------------------------------------------------------
#
# OPTIONS DERIVED FROM THE CLI
#
vCLI_USAGE_ALLOWS_CMDS=()
vCLI_USAGE_ALLOWS_OPTIONS=()
vCLI_PARSED_CMD=""
vCLI_PARSED_OPTIONS=()
vCLI_OPT_HARD_RESET=false
vCLI_OPT_CLEAR_CACHE=false
vCLI_OPT_TAG=""
vCLI_OPT_DIR=""
# --------------------------------------------------------------------------------------------
#
# config that get passed to each environment.
#
# shellcheck disable=SC2034
vENV_OPENAI_API_KEY=""
# shellcheck disable=SC2034
vENV_VULTR_API_KEY=""
# shellcheck disable=SC2034
vENV_CLOUDFLARE_API_TOKEN=""
# shellcheck disable=SC2034
vENV_GITHUB_TOKEN=""
# shellcheck disable=SC2034
vENV_IP=""
# shellcheck disable=SC2034
vENV_VULTR_S3_HOST=""
# shellcheck disable=SC2034
vENV_VULTR_S3_OBJECT_STORE=""
# shellcheck disable=SC2034
vENV_VULTR_S3_ACCESS_KEY=""
# shellcheck disable=SC2034
vENV_VULTR_S3_SECRET=""
# shellcheck disable=SC2034
vENV_GITHUB_USERNAME=""
# shellcheck disable=SC2034
vENV_GITHUB_EMAIL=""
# shellcheck disable=SC2034
vENV_CAPROVER_PASSWORD=""
# shellcheck disable=SC2034
vENV_POSTGRES_PASSWORD=""
# shellcheck disable=SC2034
vENV_CAPROVER_NAME="interbolt"
# shellcheck disable=SC2034
vENV_ROOT_DOMAIN="interbolt.org"
# shellcheck disable=SC2034
vENV_CAPROVER_SUBDOMAIN="server"
# shellcheck disable=SC2034
vENV_DEBIAN_NODE_VERSION="20.11.1"
# shellcheck disable=SC2034
vENV_DB_USER="solos"
# shellcheck disable=SC2034
vENV_DB_NAME="manager"
# shellcheck disable=SC2034
vENV_DB_PORT=5432
# shellcheck disable=SC2034
vENV_SOLOS_ID=""
# --------------------------------------------------------------------------------------------
#
# TRAPPING LOGIC
#
# This is where we set up the error trapping logic.
# When I first created this all it did was log global variables.
#
if ! declare -f utils.exit_trap >/dev/null; then
  log.error "utils.exit_trap is not a defined function. Exiting."
  exit 1
fi
trap "utils.exit_trap" EXIT
# --------------------------------------------------------------------------------------------
#
# STATE GETTERS/SETTERS
#
# These functions are used to operate on global state.
# EX: if we decide we need some value to persist between runs, we can
# easily modify the getter/setter implentation to use a file as a cache
#
state.map_parsed_cli() {
  if [ -z "$vCLI_PARSED_CMD" ]; then
    log.error "No command supplied. Please supply a command."
    exit 1
  fi
  for i in "${!vCLI_PARSED_OPTIONS[@]}"; do
    case "${vCLI_PARSED_OPTIONS[$i]}" in
    "hard-reset")
      vCLI_OPT_HARD_RESET=true
      log.debug "set \$vCLI_OPT_HARD_RESET= $vCLI_OPT_HARD_RESET"
      ;;
    dir=*)
      val="${vCLI_PARSED_OPTIONS[$i]#*=}"
      if [ "$val" == "$HOME" ]; then
        log.error "Danger: --dir flag cannot be set to the home directory. Exiting."
        exit 1
      fi
      if [[ "$HOME" == "$val"* ]]; then
        log.error "Danger: --dir flag cannot be set to a parent directory of the home directory. Exiting."
        exit 1
      fi
      vCLI_OPT_DIR="$val"
      log.debug "set \$vCLI_OPT_DIR= $vCLI_OPT_DIR"
      ;;
    tag=*)
      val="${vCLI_PARSED_OPTIONS[$i]#*=}"
      if [ -n "$val" ]; then
        vCLI_OPT_TAG="$val"
        log.debug "set \$vCLI_OPT_TAG= $vCLI_OPT_TAG"
      fi
      ;;
    "--clear-cache")
      vCLI_OPT_CLEAR_CACHE=true
      log.debug "set \$vCLI_OPT_CLEAR_CACHE= $vCLI_OPT_CLEAR_CACHE"
      ;;
    esac
  done
  if [ -z "$vCLI_OPT_DIR" ]; then
    vCLI_OPT_DIR="$(cache.get "checkout_dir")"
    if [ -z "$vCLI_OPT_DIR" ]; then
      log.error "No directory supplied or checked out in the cache. Please supply a --dir."
      exit 1
    fi
  fi
}
# --------------------------------------------------------------------------------------------
#
# COMMAND ENTRY FUNCTIONS
#
cmd.checkout() {
  precheck.throw_if_dangerous_dir
  precheck.throw_if_missing_installed_commands
  cache.set "checkout_dir" "$vCLI_OPT_DIR"

  log.info "checked out dir: $vCLI_OPT_DIR"

  if [ -f "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME" ]; then
    vENV_SOLOS_ID="$(cat "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME")"
    log.debug "set \$vENV_SOLOS_ID= $vENV_SOLOS_ID"
  fi
}
cmd.launch() {
  cmd.checkout

  if [ "$vCLI_OPT_HARD_RESET" == true ]; then
    #
    # Will throw on a dir path that is either non-existent OR
    # doesn't have a .solos file.
    #
    precheck.throw_on_nonsolos
    log.warn "DELETING: ${vCLI_OPT_DIR} in 5 seconds."
    sleep 3
    log.warn "DELETING: ${vCLI_OPT_DIR} in 2 seconds."
    sleep 2
    log.warn "DELETING: ${vCLI_OPT_DIR} here we go..."
    sleep 1
    rm -rf "$vCLI_OPT_DIR"
    log.warn "wiped and created empty dir: $vCLI_OPT_DIR"
  fi
  #
  # Will only throw on a dir path that already exists AND
  # doesn't have a .solos file. Doesn't care about non-existent dirs
  # since those will just result in new solos projects.
  #
  precheck.throw_on_nonsolos_dir
  if [ ! -d "$vCLI_OPT_DIR" ]; then
    mkdir -p "$vCLI_OPT_DIR"
    log.info "created: $vCLI_OPT_DIR"
    openssl rand -base64 32 | tr -dc 'a-z0-9' | head -c 32 >"$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME"
    log.debug "created: $vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME"
  fi
  if [ -f "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME" ] && [ -z "$vENV_SOLOS_ID" ]; then
    vENV_SOLOS_ID="$(cat "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME")"
    log.debug "set \$vENV_SOLOS_ID= $vENV_SOLOS_ID"
  fi
  #
  # I like having a status but admittedly it's only role is to warn
  # the user if they're about to run the launch command twice in a row.
  #
  last_successful_run="$(status.get "$vSTATUS_LAUNCH_SUCCEEDED")"
  if [ -n "$last_successful_run" ]; then
    log.warn "the last successful run was at: $last_successful_run"
  fi
  #
  # Generate and collect things like the caprover password, postgres passwords.
  # api keys, etc.
  # Note: do not regenerate new passwords on subsequent runs unless we explicitly break
  # the cache or a force a hard reset.
  #
  local expects_these_things=(
    "vENV_CAPROVER_PASSWORD"
    "vENV_POSTGRES_PASSWORD"
    "vENV_GITHUB_USERNAME"
    "vENV_GITHUB_EMAIL"
    "vENV_GITHUB_TOKEN"
    "vENV_OPENAI_API_KEY"
    "vENV_VULTR_API_KEY"
    "vENV_CLOUDFLARE_API_TOKEN"
  )
  # ------------------------------------------------------------------------------------------------------------
  vENV_CAPROVER_PASSWORD="$(cache.overwrite_on_empty "vENV_CAPROVER_PASSWORD" "$(openssl rand -base64 32 | tr -dc 'a-z0-9' | head -c 32)")"
  log.debug "set \$vENV_CAPROVER_PASSWORD= $vENV_CAPROVER_PASSWORD"
  # ------------------------------------------------------------------------------------------------------------
  vENV_POSTGRES_PASSWORD="$(cache.overwrite_on_empty "vENV_POSTGRES_PASSWORD" "$(openssl rand -base64 32 | tr -dc 'a-z0-9' | head -c 32)")"
  log.debug "set \$vENV_POSTGRES_PASSWORD= $vENV_POSTGRES_PASSWORD"
  # ------------------------------------------------------------------------------------------------------------
  vENV_GITHUB_USERNAME="$(cache.overwrite_on_empty "vENV_GITHUB_USERNAME" "$(git config -l | grep user.name | cut -d = -f 2)")"
  log.debug "set \$vENV_GITHUB_USERNAME= $vENV_GITHUB_USERNAME"
  # ------------------------------------------------------------------------------------------------------------
  vENV_GITHUB_EMAIL="$(cache.overwrite_on_empty "vENV_GITHUB_EMAIL" "$(git config -l | grep user.email | cut -d = -f 2)")"
  log.debug "set \$vENV_GITHUB_EMAIL= $vENV_GITHUB_EMAIL"
  # ------------------------------------------------------------------------------------------------------------
  vENV_GITHUB_TOKEN="$(cache.prompt "vENV_GITHUB_TOKEN")"
  log.debug "set \$vENV_GITHUB_TOKEN= $vENV_GITHUB_TOKEN"
  # ------------------------------------------------------------------------------------------------------------
  vENV_OPENAI_API_KEY="$(cache.prompt "vENV_OPENAI_API_KEY")"
  log.debug "set \$vENV_OPENAI_API_KEY= $vENV_OPENAI_API_KEY"
  # ------------------------------------------------------------------------------------------------------------
  vENV_VULTR_API_KEY="$(cache.prompt "vENV_VULTR_API_KEY")"
  log.debug "set \$vENV_VULTR_API_KEY= $vENV_VULTR_API_KEY"
  # ------------------------------------------------------------------------------------------------------------
  vENV_CLOUDFLARE_API_TOKEN="$(cache.prompt "vENV_CLOUDFLARE_API_TOKEN")"
  log.debug "set \$vENV_CLOUDFLARE_API_TOKEN= $vENV_CLOUDFLARE_API_TOKEN"
  # ------------------------------------------------------------------------------------------------------------
  for i in "${!expects_these_things[@]}"; do
    if [ -z "${!expects_these_things[$i]}" ]; then
      log.error "${expects_these_things[$i]} is empty. Exiting."
      exit 1
    fi
  done
  for file in "${vSTATIC_REPO_BOOTFILES[@]}"; do
    rm -f "$vCLI_OPT_DIR/bootfiles/$file"
    wget --quiet --header="Cache-Control: no-cache" "https://raw.githubusercontent.com/InterBolt/solos/main/$vSTATIC_REPO_BOOTFILES_DIR/$file" -O "$vCLI_OPT_DIR/bootfiles/$file"
    if [ "$(head -n 1 "$vCLI_OPT_DIR/bootfiles/$file" | cut -c1-2)" == "#!" ]; then
      chmod +x "$vCLI_OPT_DIR/bootfiles/$file"
    fi
    #
    # throw and remove any files that don't have valid variables.
    #
    if ! utils.template_variables "$vCLI_OPT_DIR/bootfiles/$file" "dry" 2>&1; then
      log.exit "bad variables used in: $vCLI_OPT_DIR/bootfiles/$file"
      exit 1
    fi
    log.info "downloaded: $vCLI_OPT_DIR/bootfiles/$file"
  done
  if [ ! -d "$(ssh.path.self)" ]; then
    mkdir -p "$(ssh.path.self)"
    log.info "created: $(ssh.path.self)"
    ssh-keygen -t rsa -q -f "$(ssh.path_privkey.self)" -N ""
    log.info "created private key: $(ssh.path_privkey.self), public key: $(ssh.path_pubkey.self)"
    cat "$(ssh.path_pubkey.self)" >"$(ssh.path_authorized_keys.self)"
    log.info "created $(ssh.path_authorized_keys.self)"
  fi
  chmod 644 "$(ssh.path_authorized_keys.self)"
  log.debug "updated permissions: chmod 644 - $(ssh.path_authorized_keys.self)"
  chmod 644 "$(ssh.path_pubkey.self)"
  log.debug "updated permissions: chmod 644 - $(ssh.path_pubkey.self)"
  chmod 644 "$(ssh.path_config.self)"
  log.debug "updated permissions: chmod 644 - $(ssh.path_config.self)"
  chmod 600 "$(ssh.path_privkey.self)"
  log.debug "updated permissions: chmod 600 - $(ssh.path_privkey.self)"
  #
  # Now that our SSH keys are ready, let us provision vultr stuff.
  # It's possible that that stuff is already provisioned, in which case
  # the provisioning subtasks should return early and would mean we won't have a ip to purge.
  #
  vultr.s3.provision
  log.success "vultr object storage is ready"
  #
  # prev_id is NOT the same as old_ip.
  # when prev_id is set and is associated with a matching
  # ssh key, we "promote" it to vENV_IP and skip
  # much (or all) of the vultr provisioning process.
  #
  if [ -n "${prev_ip}" ]; then
    log.info "the ip: $prev_ip from a previous run was found. will try not to re-install vultr instances."
  fi
  vultr.compute.provision "$prev_ip"
  log.success "vultr compute is ready"
  if [ -z "$vENV_IP" ]; then
    log.error "something went wrong. \$vENV_IP is empty."
    exit 1
  fi
  if [ "$vENV_IP" != "$prev_ip" ]; then
    cache.set "old_ip" "$prev_ip"
  else
    #
    # If we detected that the ip did NOT chang after vultr provisioning, we must
    # clear the current old_ip value. Not doing so would cause the
    # the current vultr instance to be deleted at the end of this script.
    #
    cache.del "old_ip"
  fi
  #
  # vENV_ variables => .env file and .env.sh files
  #
  environment.generate_env_files
  {
    echo "Host 127.0.0.1"
    echo "  HostName 127.0.0.1"
    echo "  User root"
    echo "  IdentityFile $(ssh.path_privkey.self)"
    echo "  Port 2222"
    echo ""
    echo ""
    echo "Host $vENV_IP"
    echo "  HostName $vENV_IP"
    echo "  User root"
    echo "  IdentityFile $(ssh.path_privkey.self)"
  } >"$(ssh.path_config.self)"
  log.info "created: $(ssh.path_config.self)."
  #
  # Checkpoint: once we're here, it's reasonable to assume
  # that any variables we need to be set are set. That means
  # we can inject them into downloaded/generated source files.
  #
  utils.template_variables "$vCLI_OPT_DIR" "commit"
  log.info "injected vars into source files."
  #
  # Build and start the local docker container.
  # Warning: this will recreate the container on each run.
  # Mounts the solos dir into the container.
  # The command should look like this:
  # "docker compose --file compose.yml up --force-recreate --build --remove-orphans --detach"
  # And we need to get the container id so we can perform future operations on it like
  # are you running, are you bootstrapped, etc.
  #
  COMPOSE_PROJECT_NAME="solos-$vENV_SOLOS_ID" docker compose --file compose.yml up --force-recreate --build --remove-orphans --detach
  log.info "docker container is ready"
  #
  # Remember: the docker container already has the env files at
  # the /root/workspace directory, where we mount our solos dir.
  #
  ssh.rsync_up.remote "$vCLI_OPT_DIR/$vSTATIC_ENV_FILENAME" "/root/"
  ssh.rsync_up.remote "$vCLI_OPT_DIR/$vSTATIC_ENV_SH_FILENAME" "/root/"
  log.info "uploaded env files to both local and remote."

  ssh.rsync_up.remote "$$vCLI_OPT_DIR/$vSTATIC_BOOTSTRAP_SH" "/root/"
  ssh.cmd.remote "chmod +x /root/$vSTATIC_BOOTSTRAP_SH"
  log.info "uploaded and set permissions for remote bootstrap script."
  #
  # Create the folder where we'll store out caprover
  # deployment tar files.
  #
  ssh.cmd.remote "mkdir -p /root/deployments"
  log.info "created remote deployment dir: /root/deployments"
  #
  # Before bootstrapping can occur, make sure we upload the .solos config folder
  # from our local machine to the remote machine.
  # Important: we don't need to do this with the docker container because we rely on mounting
  # the local .solos folder to the container.
  #
  if ssh.cmd.remote '[ -d '"${vSTATIC_DEBIAN_CONFIG_ROOT}"' ]'; then
    log.warn "remote already has a .solos config folder. skipping. see \`solos --help\` for more options."
  else
    ssh.cmd.remote "mkdir -p ${vSTATIC_DEBIAN_CONFIG_ROOT}"
    log.info "created empty remote .solos config folder."
    ssh.rsync_up.remote "$vSTATIC_MY_CONFIG_ROOT/" "$vSTATIC_DEBIAN_CONFIG_ROOT/"
    log.info "uploaded local .solos config folder to remote."
  fi
  #
  #
  # Note: the bootstrapping script should be idempotent but it's still
  # a long ass script so I like the ability to skip it. Also, the downsides of not re-running
  # are mitigated by the fact that every script that contributes to the bootstrapping
  # is available via an aliased command.
  #
  ssh.cmd.remote "/root/$vSTATIC_BOOTSTRAP_SH remote"
  #
  # I'm not doing anything with this status for now.
  # because I prefer that the bootstrapping script take
  # responsibility for idempotency and performance.
  #
  status.set "$vSTATUS_BOOTSTRAPPED_REMOTE" "$(utils.date)"
  log.info "bootstrapped the remote server."
  #
  # Bootstrapping postgres is a manual process so that's why I skip if
  # it was already created.
  #
  timestamp_of_postgres_bootstrap="$(status.get "$vSTATUS_BOOTSTRAPPED_POSTGRES")"
  if [ -n "$timestamp_of_postgres_bootstrap" ]; then
    log.warn "skipping all postgres bootstrap - previously completed at $timestamp_of_postgres_bootstrap"
  else
    ssh.rsync_down.remote "$vSTATIC_DEBIAN_CLONE_DIR/$vSTATIC_DB_ONE_CLICK_TEMPLATE_FILENAME" "$vCLI_OPT_DIR/"
    log.info "downloaded caprover one-click-app template."
    utils.echo_line
    echo ""
    cat "$vCLI_OPT_DIR/$vSTATIC_DB_ONE_CLICK_TEMPLATE_FILENAME"
    echo ""
    utils.echo_line
    echo "Setup the caprover Postgres database and hit enter."
    read -r
    status.set "$vSTATUS_BOOTSTRAPPED_POSTGRES" "$(utils.date)"
  fi
  #
  # Make sure the local bin executable is up to date.
  #
  ssh.ssh.rsync_up.docker "$vSTATIC_USR_LOCAL_BIN_EXECUTABLE" "/usr/local/bin/"
  #
  # The logic here is simpler because the bootstrap script for the docker container
  # will never deal with things like databases or service orchestration.
  #
  ssh.cmd.docker "$STORE_TARGET_NAMED_BOOTSTRAP_SH_FILE docker"
  status.set "$vSTATUS_BOOTSTRAPPED_DOCKER" "$(utils.date)"
  log.info "bootstrapped the local docker container."
  #
  # This is redundant, but it's a good safety check because
  # if something bad happened and the old ip is the same as the current
  # we'll end up destroying the current instance. Yikes.
  #
  if [ "$(cache.get "old_ip")" == "$vENV_IP" ]; then
    log.error "The old and active ip's should never be the same! Skipping to avoid a disaster."
    exit 1
  fi
  #
  # The active ip should never be empty.
  #
  if [ -z "$vENV_IP" ]; then
    log.error "expected \$vENV_IP to be non-empty. Exiting"
    exit 1
  fi
  if [ -z "$(cache.get "old_ip")" ]; then
    #
    # Common if the user is installing for the first time.
    #
    log.info "no old ip found. skipping the purge process."
  else
    #
    # If the user is re-installing, we want to make sure we don't leave behind an errant ip addresses throughout
    # the installation folder, or that we don't leave dangling vultr resources
    #
    find "$vCLI_OPT_DIR" -type f -exec sed -i 's/'"$(cache.get "old_ip")"'/'"${vENV_IP}"'/g' {} \;
    log.info "replaced $(cache.get "old_ip") with $vENV_IP in all files for extra safety."
    log.warn "waiting 5 seconds before destroying the previous instance!"
    sleep 5
    vultr.compute.destroy_instance "$(vultr.compute.get_instance_id_from_ip "$(cache.get "old_ip")")"
    log.info "destroyed the previous instance with ip: $(cache.get "old_ip")"
  fi
  status.set "$vSTATUS_LAUNCH_SUCCEEDED" "$(utils.date)"

  log.success "launch completed successfully."
}
cmd.sync_config() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  if ! ssh.cmd.remote '[ -d '"${vSTATIC_DEBIAN_CONFIG_ROOT}"' ]'; then
    log.error "remote does not have a config folder. are you sure this server is launched? Exiting."
  fi
  log.warn "will overwrite the remote config folder in 5 seconds."
  sleep 3
  log.warn "will overwrite the remote config folder in 2 seconds."
  sleep 2
  log.warn "will overwrite the remote config folder in 1 second."
  sleep 1
  #
  # Do not rm -rf in the first command because we want as little downtime as possible.
  # instead, forcefully move the rsync'd local config folder to overwrite the remote one.
  #
  local tmp_dir="/root/.tmp"
  local config_dirname=".solos"
  ssh.cmd.remote "mkdir -p  ${tmp_dir}"
  log.info "remote tmp folder exists"
  ssh.cmd.remote "mkdir -p ${tmp_dir}/${config_dirname} && rm -rf ${tmp_dir}/${config_dirname}"
  log.info "wiped remote tmp/${config_dirname} folder in preparation for rsync."
  ssh.rsync_up.remote "$vSTATIC_MY_CONFIG_ROOT/" "${tmp_dir}/${config_dirname}/"
  log.info "uploaded local ${config_dirname} config folder to remote."
  ssh.cmd.remote "mv --force ${tmp_dir}/${config_dirname} ${vSTATIC_DEBIAN_CONFIG_ROOT}"
  log.info "overwrote remote ${config_dirname} config folder with our local one."
  ssh.cmd.remote "rm -rf ${tmp_dir}"
  log.info "cleaned up remote tmp folder."

  log.info "success: uploaded local .solos config folder to remote."
}
cmd.sync_bin() {
  cmd.checkout

  if [ "$vSTATIC_HOST" != "local" ]; then
    log.error "this command must be run from the local host. Exiting."
    exit 1
  fi
  precheck.docker_host_running
  ssh.ssh.rsync_up.docker "$vSTATIC_USR_LOCAL_BIN_EXECUTABLE" "/usr/local/bin/"
}
cmd.sync_env() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "remote" ]; then
    log.error "this command cannot be run from the remote host. Exiting."
    exit 1
  fi
  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  log.warn "will overwrite the remote .solos config folder in 5 seconds."
  sleep 3
  log.warn "will overwrite the remote .solos config folder in 2 seconds."
  sleep 2
  log.warn "will overwrite the remote .solos config folder in 1 second."
  sleep 1
  #
  # Do not rm -rf in the first command because we want as little downtime as possible.
  # instead, forcefully move the rsync'd local config folder to overwrite the remote one.
  #
  local tmp_dir="/root/.tmp"
  local local_env=$vCLI_OPT_DIR/$vSTATIC_ENV_FILENAME
  local local_env_sh=$vCLI_OPT_DIR/$vSTATIC_ENV_SH_FILENAME
  local tmp_env="$tmp_dir/$vSTATIC_ENV_SH_FILENAME"
  local tmp_env_sh="$tmp_dir/$vSTATIC_ENV_FILENAME"
  local target_env="/root/$vSTATIC_ENV_FILENAME"
  local target_env_sh="/root/$vSTATIC_ENV_SH_FILENAME"
  ssh.cmd.remote "mkdir -p  $tmp_dir"
  log.info "remote tmp folder exists"
  ssh.cmd.remote "rm -f ${tmp_env} && rm -f ${tmp_env_sh}"
  log.info "cleaned remote tmp dir in preparation for rsync."
  ssh.rsync_up.remote "$local_env" "$tmp_dir/"
  log.info "uploaded .env file to remote tmp folder."
  ssh.rsync_up.remote "$local_env_sh" "$tmp_dir/"
  log.info "uploaded .env.sh file to remote tmp folder."
  ssh.cmd.remote "mv --force $tmp_env_sh $target_env_sh && mv --force $tmp_env $target_env"
  log.info "overwrote remote .solos config folder with our local one."
  ssh.cmd.remote "rm -rf $tmp_dir"
  log.info "cleaned up remote tmp folder."

  log.info "success: uploaded local .env and .env.sh files to remote."
}
cmd.code() {
  cmd.checkout

  if ! command -v "code" &>/dev/null; then
    log.error "vscode is not installed to your path. cannot continue."
  fi
  if [ "$vSTATIC_HOST" != "local" ]; then
    log.error "this command must be run from the local host. Exiting."
    exit 1
  fi
  precheck.docker_host_running

  log.warn "would open vscode"
}
cmd.restore() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  log.warn "TODO: implementation needed"
}
cmd.backup() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  log.warn "TODO: implementation needed"
}
cmd.status() {
  local status_solos_id=""
  local status_container_runninng="NO"
  local status_launched="NO"
  local status_docker_ready="NO"
  local status_remote_ready="NO"
  local status_postgres_ready="NO"
  if [ -z "$vCLI_OPT_DIR" ]; then
    vCLI_OPT_DIR="$(cache.get "checkout_dir")"
  fi
  if [ -z "$vCLI_OPT_DIR" ]; then
    log.error "no project found at: $vCLI_OPT_DIR. if you moved it, use the --dir flag to specify the new location."
    exit 1
  fi
  if [ ! -f "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME" ]; then
    log.error "no project id found at: $vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME. this should never happen."
    exit 1
  fi
  #
  # If we're here, we know we have a project directory.
  # Let's go ahead and throw errors when base assumptions
  # like having a config dir are not met.
  #
  status_solos_id="$(cat "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME")"
  if [ -z "$status_solos_id" ]; then
    log.error "no solos id found at: $vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME. Exiting."
    exit 1
  fi
  if [ ! -d "$vSTATIC_MY_CONFIG_ROOT" ]; then
    log.error "no config dir found at: $vSTATIC_MY_CONFIG_ROOT. Exiting."
    exit 1
  fi
  #
  # Collect the status of the project.
  #
  if [ -n "$(status.get "$vSTATUS_LAUNCH_SUCCEEDED")" ]; then
    status_launched="YES"
  fi
  if [ -n "$(status.get "$vSTATUS_BOOTSTRAPPED_REMOTE")" ]; then
    status_remote_ready="YES"
  fi
  if [ -n "$(status.get "$vSTATUS_BOOTSTRAPPED_POSTGRES")" ]; then
    status_postgres_ready="YES"
  fi
  if [ -n "$(status.get "$vSTATUS_BOOTSTRAPPED_DOCKER")" ]; then
    status_docker_ready="YES"
  fi
  if precheck.docker_host_running &>/dev/null; then
    status_container_runninng="YES"
  fi
  log.info "CONFIG_DIR: $status_config_dir"
  log.info "DIR: $vCLI_OPT_DIR"
  log.info "STATUS: docker running: $status_container_runninng"
  log.info "PROJECT_ID: $status_solos_id"
  log.info "STATUS: fully launched: $status_launched"
  log.info "STATUS: bootstrapped remote: $status_remote_ready"
  log.info "STATUS: bootstrapped postgres: $status_postgres_ready"
  log.info "STATUS: bootstrapped docker: $status_docker_ready"
}
cmd.test() {
  cmd.checkout

  if [ "$vSTATIC_RUNNING_IN_GIT_REPO" == "true" ] && [ "$vSTATIC_HOST" == "local" ]; then
    test.variables
    test.bootfiles
  else
    log.error "this command can only be run from within a git repo."
    exit 1
  fi
}
# --------------------------------------------------------------------------------------------
#
# ENTRY
#
# parse the cli args and validate them.
#
flags.parse_requirements
flags.parse_cmd "$@"
flags.validate_options
#
# We seperate the parsing and mapping concerns so that
# frequent changes to business logic don't affect the parsing logic
#
state.map_parsed_cli
if [ -z "$vCLI_OPT_DIR" ]; then
  log.error "--dir is required. Exiting."
  exit 1
fi
#
# Before doing ANYTHING, check that our command actually works.
# Fail fast!
#
if ! command -v "cmd.$vCLI_PARSED_CMD" &>/dev/null; then
  log.error "cmd.$vCLI_PARSED_CMD is not defined. Exiting."
  exit 1
fi
#
# We're only allowed to clear cache files associated with a particular named installation.
#
if [ "$vCLI_OPT_CLEAR_CACHE" == "true" ]; then
  state.clear.cache
fi
#
# Run the command specified in the cli args.
#
"cmd.$vCLI_PARSED_CMD"
