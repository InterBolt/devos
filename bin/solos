#!/usr/bin/env bash
# shellcheck disable=SC2115
set -o errexit
set -o pipefail
set -o errtrace

cd "$(dirname "${BASH_SOURCE[0]}")"
vBIN_DIR="$(pwd)"
vBIN_FILEPATH="$vBIN_DIR/$0"
vDEBUG_LEVEL=${DEBUG_LEVEL:-0}
#
# These variables are shared between this script AND every helper script
# in the installer/bin directory.
#
# shellcheck source=solos.static
. "solos.static"
#
# `dotglob` option ensures that dotfiles and folders are included when using globs.
# Helpful for looping through files in a directory.
#
shopt -s dotglob
# --------------------------------------------------------------------------------------------
#
# RESPONSE/RETURN SLOTS
#
vPREV_CURL_RESPONSE=""
vPREV_CURL_ERR_STATUS_CODE=""
vPREV_CURL_ERR_MESSAGE=""
vPREV_RETURN=()
# --------------------------------------------------------------------------------------------
#
# STATUSES:
#
# These are used to keep track of various statuses, like "did the bootstrap complete"
#
vSTATUS_BOOTSTRAPPED_POSTGRES="bootstrapped-remote-postgres"
vSTATUS_BOOTSTRAPPED_REMOTE="bootstrapped-remote"
vSTATUS_LAUNCH_SUCCEEDED="successful-run"
vSTATUS_BOOTSTRAPPED_DOCKER="bootstrapped-docker"
# --------------------------------------------------------------------------------------------
#
# SOURCE SCRIPTS
#
# Think of these like libraries.
# Except they are tightly coupled to the variables defined in this script.
#
# shellcheck source=shared.log.sh
. "shared.log.sh"
log.ready "cli"
#
# Now source all the things we need.
#
# shellcheck source=solos.cache
. "solos.cache"
# shellcheck source=solos.flags
. "solos.flags"
# shellcheck source=solos.ssh
. "solos.ssh"
# shellcheck source=solos.status
. "solos.status"
# shellcheck source=solos.utils
. "solos.utils"
# shellcheck source=solos.vultr
. "solos.vultr"
# shellcheck source=solos.precheck
. "solos.precheck"
# shellcheck source=solos.environment
. "solos.environment"
# shellcheck source=solos.test
. "solos.test"
#
# Return to the previous directory just in case.
#
cd "$vBIN_DIR"
#
# --------------------------------------------------------------------------------------------
#
# OPTIONS DERIVED FROM THE CLI
#
vCLI_USAGE_ALLOWS_CMDS=()
vCLI_USAGE_ALLOWS_OPTIONS=()
vCLI_PARSED_CMD=""
vCLI_PARSED_OPTIONS=()
vCLI_OPT_HARD_RESET=false
vCLI_OPT_CLEAR_CACHE=false
vCLI_OPT_TAG=""
vCLI_OPT_DIR=""
vCLI_OPT_SERVER=""
# --------------------------------------------------------------------------------------------
#
# config that get passed to each environment.
#
# shellcheck disable=SC2034
vENV_OPENAI_API_KEY=""
# shellcheck disable=SC2034
vENV_VULTR_API_KEY=""
# shellcheck disable=SC2034
vENV_CLOUDFLARE_API_TOKEN=""
# shellcheck disable=SC2034
vENV_GITHUB_TOKEN=""
# shellcheck disable=SC2034
vENV_IP=""
# shellcheck disable=SC2034
vENV_VULTR_S3_HOST=""
# shellcheck disable=SC2034
vENV_VULTR_S3_OBJECT_STORE=""
# shellcheck disable=SC2034
vENV_VULTR_S3_ACCESS_KEY=""
# shellcheck disable=SC2034
vENV_VULTR_S3_SECRET=""
# shellcheck disable=SC2034
vENV_GITHUB_USERNAME=""
# shellcheck disable=SC2034
vENV_GITHUB_EMAIL=""
# shellcheck disable=SC2034
vENV_SEED_SECRET=""
# shellcheck disable=SC2034
vENV_CAPROVER_NAME="interbolt"
# shellcheck disable=SC2034
vENV_ROOT_DOMAIN="interbolt.org"
# shellcheck disable=SC2034
vENV_CAPROVER_SUBDOMAIN="server"
# shellcheck disable=SC2034
vENV_DEBIAN_NODE_VERSION="20.11.1"
# shellcheck disable=SC2034
vENV_DB_USER="solos"
# shellcheck disable=SC2034
vENV_DB_NAME="manager"
# shellcheck disable=SC2034
vENV_DB_PORT=5432
# shellcheck disable=SC2034
vENV_SOLOS_ID=""
# --------------------------------------------------------------------------------------------
#
# TRAPPING LOGIC
#
# This is where we set up the error trapping logic.
# When I first created this all it did was log global variables.
#
if ! declare -f utils.exit_trap >/dev/null; then
  log.error "utils.exit_trap is not a defined function. Exiting."
  exit 1
fi
trap "utils.exit_trap" EXIT
# --------------------------------------------------------------------------------------------
#
# Utility functions that don't yet have their own categories, so we prefix
# them with solos.* to keep them organized and separate from the rest of the lib
# functions.
#
solos.map_parsed_cli() {
  local was_server_set=false
  if [ -z "$vCLI_PARSED_CMD" ]; then
    log.error "No command supplied. Please supply a command."
    exit 1
  fi
  for i in "${!vCLI_PARSED_OPTIONS[@]}"; do
    case "${vCLI_PARSED_OPTIONS[$i]}" in
    "hard-reset")
      vCLI_OPT_HARD_RESET=true
      log.debug "set \$vCLI_OPT_HARD_RESET= $vCLI_OPT_HARD_RESET"
      ;;
    dir=*)
      val="${vCLI_PARSED_OPTIONS[$i]#*=}"
      if [ "$val" == "$HOME" ]; then
        log.error "Danger: --dir flag cannot be set to the home directory. Exiting."
        exit 1
      fi
      if [[ "$HOME" == "$val"* ]]; then
        log.error "Danger: --dir flag cannot be set to a parent directory of the home directory. Exiting."
        exit 1
      fi
      vCLI_OPT_DIR="$val"
      log.debug "set \$vCLI_OPT_DIR= $vCLI_OPT_DIR"
      ;;
    server=*)
      val="${vCLI_PARSED_OPTIONS[$i]#*=}"
      vCLI_OPT_SERVER="$val"
      was_server_set=true
      log.debug "set \$vCLI_OPT_SERVER= $vCLI_OPT_SERVER"
      ;;
    tag=*)
      val="${vCLI_PARSED_OPTIONS[$i]#*=}"
      if [ -n "$val" ]; then
        vCLI_OPT_TAG="$val"
        log.debug "set \$vCLI_OPT_TAG= $vCLI_OPT_TAG"
      fi
      ;;
    "--clear-cache")
      vCLI_OPT_CLEAR_CACHE=true
      log.debug "set \$vCLI_OPT_CLEAR_CACHE= $vCLI_OPT_CLEAR_CACHE"
      ;;
    esac
  done
  if [ "$was_server_set" == false ]; then
    vCLI_OPT_SERVER="$(cache.get "checkout_server")"
    if [ -z "$vCLI_OPT_SERVER" ]; then
      vCLI_OPT_SERVER="$vSTATIC_DEFAULT_SERVER"
      log.debug "set \$vCLI_OPT_SERVER= $vCLI_OPT_SERVER"
    fi
  fi
  if [ -z "$vCLI_OPT_DIR" ]; then
    vCLI_OPT_DIR="$(cache.get "checkout_dir")"
    if [ -z "$vCLI_OPT_DIR" ]; then
      log.error "No directory supplied or checked out in the cache. Please supply a --dir."
      exit 1
    fi
    log.debug "set \$vCLI_OPT_DIR= $vCLI_OPT_DIR"
  fi
}
solos.rebuild_launch_directory() {
  #
  # Important: we want to approach the files inside of the launch
  # dir as somewhat ephemeral and not worry about overwriting them.
  # This is helpful too in future proofing the script against changes
  # to the --dir flag.
  #
  local server_boot_dir="$vCLI_OPT_DIR/repo/servers/$vCLI_OPT_SERVER/$vSTATIC_BOOT_DIR"
  local templates_dir="$vCLI_OPT_DIR/repo/$vSTATIC_REPO_TEMPLATES_DIR"
  local target_launch_dir="$vCLI_OPT_DIR/$vSTATIC_PROJECT_LAUNCH_DIR"
  local tmp_launch_dir="$vCLI_OPT_DIR/.tmp/$vSTATIC_PROJECT_LAUNCH_DIR"
  if [ -d "$target_launch_dir" ]; then
    log.warn "rebuilding the launch directory."
  fi
  rm -rf "$tmp_launch_dir"
  mkdir -p "$tmp_launch_dir"
  cp -a "$server_boot_dir/." "$tmp_launch_dir/"
  cp -a "$templates_dir/." "$tmp_launch_dir/"
  if ! utils.template_variables "$tmp_launch_dir" "commit" 2>&1; then
    log.exit "something unexpected happened while injecting variables into the launch directory."
    exit 1
  fi
  rm -rf "$target_launch_dir"
  log.debug "deleted: $target_launch_dir"
  mv "$tmp_launch_dir" "$target_launch_dir"
  log.info "created launch directory: $target_launch_dir"
}
solos.import_project_repo() {
  #
  # Rather than download portions of the repo we need, we prefer
  # to rely on a full clone in each project directory.
  #
  # Note: maybe in the future, if we want to prevent re-runs of the
  # launch command from busting our version of the repo in our project
  # we can automate forking the repo on the initial clone so that pulls
  # won't cause any issues. But for now, the forking needs to occur
  # manually by the user in their workspace.
  #
  if [ -d "$vCLI_OPT_DIR/repo" ]; then
    log.warn "the SolOS repo was already cloned to $vCLI_OPT_DIR/repo."
    log.info "pulling latest changes on checked out branch."
    #
    # Use the -C option instead of cd'ing to
    # maintain working directory.
    #
    git -C "$vCLI_OPT_DIR/repo" pull 2>/dev/null
    log.info "pulled latest changes."
  else
    git clone https://github.com/InterBolt/solos.git "$vCLI_OPT_DIR/repo" 2>/dev/null
    log.info "cloned the SolOS repo to $vCLI_OPT_DIR/repo."
  fi
  if [ ! -d "$vCLI_OPT_DIR/repo/servers/$vCLI_OPT_SERVER" ]; then
    cache.del "checkout_server"
    log.debug "deleted the checkout_server cache since the server doesn't exist in the repo."
    log.error "The server $vCLI_OPT_SERVER does not exist in the SolOS repo. Exiting."
    exit 1
  fi
  find "$vCLI_OPT_DIR/repo" -type f -name "*.sh" -exec chmod +x {} \;
  find "$vCLI_OPT_DIR/repo/bin" -type f -name "solos*" -exec chmod +x {} \;
  chmod +x install
  log.info "set permissions for all shell scripts in: $vCLI_OPT_DIR/repo"
}
solos.build_project_ssh_dir() {
  #
  # This is the dir we'll use to store all the keyfiles required
  # by our local, docker dev container, and remote server.
  # Important: if a dev manually deletes this dir before re-running a launch,
  # infra will get recreated and the keys will get regenerated.
  #
  if [ ! -d "$(ssh.path.self)" ]; then
    mkdir -p "$(ssh.path.self)"
    log.info "created: $(ssh.path.self)"
    ssh-keygen -t rsa -q -f "$(ssh.path_privkey.self)" -N ""
    log.info "created private key: $(ssh.path_privkey.self), public key: $(ssh.path_pubkey.self)"
    cat "$(ssh.path_pubkey.self)" >"$(ssh.path_authorized_keys.self)"
    log.info "created $(ssh.path_authorized_keys.self)"
  fi
  chmod 644 "$(ssh.path_authorized_keys.self)"
  log.debug "updated permissions: chmod 644 - $(ssh.path_authorized_keys.self)"
  chmod 644 "$(ssh.path_pubkey.self)"
  log.debug "updated permissions: chmod 644 - $(ssh.path_pubkey.self)"
  chmod 644 "$(ssh.path_config.self)"
  log.debug "updated permissions: chmod 644 - $(ssh.path_config.self)"
  chmod 600 "$(ssh.path_privkey.self)"
  log.debug "updated permissions: chmod 600 - $(ssh.path_privkey.self)"
}
# --------------------------------------------------------------------------------------------
#
# COMMAND ENTRY FUNCTIONS
#
cmd.checkout() {
  precheck.throw_if_dangerous_dir
  precheck.throw_if_missing_installed_commands
  cache.set "checkout_dir" "$vCLI_OPT_DIR"
  cache.set "checkout_server" "$vCLI_OPT_SERVER"
  log.info "checked out dir: $vCLI_OPT_DIR"
  if [ -f "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME" ]; then
    vENV_SOLOS_ID="$(cat "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME")"
    log.debug "set \$vENV_SOLOS_ID= $vENV_SOLOS_ID"
  fi
  if [ -f "$(ssh.path_config.self)" ]; then
    #
    # For the most part we can just assume the ip we extract here
    # is the correct one. The time where it isn't true is if we wipe our project's .ssh
    # dir and re-run the launch command. In that case, the cache will tell
    # us the most recent ip before the wipe and we can use that to de-provision
    # resources as necessary.
    #
    # Important: a critical assumption is that the cache is never wiped between
    # the time we deleted the .ssh dir and the time we re-run the launch command.
    # In such a case, our script won't know what to de-provision and the user will
    # have to do that themselves through their provider's UI.
    # I think this is ok as long as clear warnings are put in place.
    #
    local most_recent_ip="$(ssh.extract_ip.remote)"
    cache.set "most_recent_ip" "$most_recent_ip"
    log.debug "updated the most recent ip in the cache."
  fi
}
cmd.launch() {
  cmd.checkout

  if [ "$vCLI_OPT_HARD_RESET" == true ]; then
    #
    # Will throw on a dir path that is either non-existent OR
    # doesn't have a .solos file.
    #
    precheck.throw_on_nonsolos
    log.warn "DELETING: ${vCLI_OPT_DIR} in 5 seconds."
    sleep 3
    log.warn "DELETING: ${vCLI_OPT_DIR} in 2 seconds."
    sleep 2
    log.warn "DELETING: ${vCLI_OPT_DIR} here we go..."
    sleep 1
    rm -rf "$vCLI_OPT_DIR"
    log.warn "wiped and created empty dir: $vCLI_OPT_DIR"
  fi
  #
  # Will only throw on a dir path that already exists AND
  # doesn't have a .solos file. Doesn't care about non-existent dirs
  # since those will just result in new solos projects.
  #
  precheck.throw_on_nonsolos_dir
  if [ ! -d "$vCLI_OPT_DIR" ]; then
    mkdir -p "$vCLI_OPT_DIR"
    log.info "created new SolOS project at: $vCLI_OPT_DIR"
    vENV_SOLOS_ID="$(utils.generate_secret)"
    echo "$vENV_SOLOS_ID" >"$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME"
    log.info "created new SolOS project at: $vCLI_OPT_DIR with id: $vENV_SOLOS_ID"
  fi
  if [ -f "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME" ]; then
    vENV_SOLOS_ID="$(cat "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME")"
    log.debug "set \$vENV_SOLOS_ID= $vENV_SOLOS_ID"
  fi
  #
  # I like having a status but admittedly it's only role is to warn
  # the user if they're about to run the launch command twice in a row.
  #
  last_successful_run="$(status.get "$vSTATUS_LAUNCH_SUCCEEDED")"
  if [ -n "$last_successful_run" ]; then
    log.warn "the last successful run was at: $last_successful_run"
  fi
  solos.import_project_repo
  #
  # Confirm any assumptions we make later in the script.
  # Ex: the specified server exists, templates are valid, server/.boot dir valid, etc.
  # We want aggressive prechecks BEFORE the ssh keygen and vultr provisioning
  # sections since those create things that are harder to undo and debug.
  #
  precheck.validate_project_repo "$vCLI_OPT_DIR/repo"
  #
  # Generate and collect things like the caprover password, postgres passwords.
  # api keys, etc.
  # Note: do not regenerate new passwords on subsequent runs unless we explicitly break
  # the cache or a force a hard reset.
  #
  local expects_these_things=(
    "vENV_SEED_SECRET"
    "vENV_GITHUB_USERNAME"
    "vENV_GITHUB_EMAIL"
    "vENV_GITHUB_TOKEN"
    "vENV_OPENAI_API_KEY"
    "vENV_VULTR_API_KEY"
    "vENV_CLOUDFLARE_API_TOKEN"
  )
  # ------------------------------------------------------------------------------------------------------------
  vENV_SEED_SECRET="$(cache.overwrite_on_empty "vENV_SEED_SECRET" "$(utils.generate_secret)")"
  log.debug "set \$vENV_SEED_SECRET= $vENV_SEED_SECRET"
  # ------------------------------------------------------------------------------------------------------------
  vENV_GITHUB_USERNAME="$(cache.overwrite_on_empty "vENV_GITHUB_USERNAME" "$(git config -l | grep user.name | cut -d = -f 2)")"
  log.debug "set \$vENV_GITHUB_USERNAME= $vENV_GITHUB_USERNAME"
  # ------------------------------------------------------------------------------------------------------------
  vENV_GITHUB_EMAIL="$(cache.overwrite_on_empty "vENV_GITHUB_EMAIL" "$(git config -l | grep user.email | cut -d = -f 2)")"
  log.debug "set \$vENV_GITHUB_EMAIL= $vENV_GITHUB_EMAIL"
  # ------------------------------------------------------------------------------------------------------------
  vENV_GITHUB_TOKEN="$(cache.prompt "vENV_GITHUB_TOKEN")"
  log.debug "set \$vENV_GITHUB_TOKEN= $vENV_GITHUB_TOKEN"
  # ------------------------------------------------------------------------------------------------------------
  vENV_OPENAI_API_KEY="$(cache.prompt "vENV_OPENAI_API_KEY")"
  log.debug "set \$vENV_OPENAI_API_KEY= $vENV_OPENAI_API_KEY"
  # ------------------------------------------------------------------------------------------------------------
  vENV_VULTR_API_KEY="$(cache.prompt "vENV_VULTR_API_KEY")"
  log.debug "set \$vENV_VULTR_API_KEY= $vENV_VULTR_API_KEY"
  # ------------------------------------------------------------------------------------------------------------
  vENV_CLOUDFLARE_API_TOKEN="$(cache.prompt "vENV_CLOUDFLARE_API_TOKEN")"
  log.debug "set \$vENV_CLOUDFLARE_API_TOKEN= $vENV_CLOUDFLARE_API_TOKEN"
  # ------------------------------------------------------------------------------------------------------------
  for i in "${!expects_these_things[@]}"; do
    if [ -z "${!expects_these_things[$i]}" ]; then
      log.error "${expects_these_things[$i]} is empty. Exiting."
      exit 1
    fi
  done
  solos.build_project_ssh_dir
  #
  # On re-runs, the vultr provisioning functions will check for the existence
  # of the old ip and if it's the same as the current ip, it will skip the
  # provisioning process.
  #
  vultr.s3.provision
  log.success "vultr object storage is ready"
  #
  # prev_id is NOT the same as ip_to_deprovision.
  # when prev_id is set and is associated with a matching
  # ssh key, we "promote" it to vENV_IP and skip
  # much (or all) of the vultr provisioning process.
  #
  local most_recent_ip="$(cache.get "most_recent_ip")"
  local ip_to_deprovision="$(cache.get "ip_to_deprovision")"
  if [ -n "${most_recent_ip}" ]; then
    log.info "the ip \`$most_recent_ip\` from a previous run was found. will not re-install vultr instance"
  fi
  vultr.compute.provision "$most_recent_ip"
  vENV_IP="${vPREV_RETURN[0]}"
  log.success "vultr compute is ready"
  #
  # I'm treating the vultr. functions as a black box and then doing
  # critical checks on the produced ip. Should throw when:
  # 1) vENV_IP is empty after provisioning
  # 2) the ip to deprovision in our cache is the same as vENV_IP
  #
  if [ -z "$vENV_IP" ]; then
    log.error "Unexpected error: the current ip is empty. Exiting."
    exit 1
  fi
  if [ "$ip_to_deprovision" == "$vENV_IP" ]; then
    log.error "Unexpected error: the ip to deprovision is the same as the current ip. Exiting."
  fi
  #
  # After the sanity checks, if the ip changed, we're safe
  # to update the cache slots for the most recent ip and the
  # ip to deprovision.
  # By putting the ip to deprovision in the cache, we ensure that
  # a hard reset won't stop our script from deprovisioning the old instance.
  # on future runs.
  #
  if [ "$vENV_IP" != "$most_recent_ip" ]; then
    cache.set "ip_to_deprovision" "$most_recent_ip"
    cache.set "most_recent_ip" "$vENV_IP"
  fi
  #
  # Generates the .env/.env.sh files by mapping all
  # global variables starting with vENV_* to both files.
  #
  environment.generate_env_files
  #
  # Builds the ssh config file for the remote server and
  # local docker dev container.
  # Important: the ssh config file is the source of truth for
  # our remote ip.
  #
  ssh.build.config "$ip"
  log.info "created: $(ssh.path_config.self)."
  #
  # Next, we want to form the launch directory inside of our project directory using the
  # bootfiles from the server/.boot directory, and the built template files from bin/templates.
  # These launch files are used later to bootstrap our environments.
  # Important: these files are ephemeral and will be rebuilt/overwritten on each run.
  #
  solos.rebuild_launch_directory
  local launch_dir="$vCLI_OPT_DIR/$vSTATIC_PROJECT_LAUNCH_DIR"
  #
  # Build and start the local docker container.
  # We set the COMPOSE_PROJECT_NAME environment variable to
  # the unique id of our project so that we can easily detect
  # whether or not a specific project's dev container is running.
  # Note: I'm being lazy and just cd'ing in and out to run the compose
  # command. This keeps the compose.yml config a little simpler.
  #
  local entry_dir="$PWD"
  cd "$launch_dir"
  COMPOSE_PROJECT_NAME="solos-$vENV_SOLOS_ID" docker compose --file compose.yml up --force-recreate --build --remove-orphans --detach
  log.info "docker container is ready"
  cd "$entry_dir"
  #
  # Remember: the docker container already has the env files at
  # the /root/workspace directory, where we mount our SolOS dir.
  #
  ssh.rsync_up.remote "$vCLI_OPT_DIR/$vSTATIC_ENV_FILENAME" "/root/"
  ssh.rsync_up.remote "$vCLI_OPT_DIR/$vSTATIC_ENV_SH_FILENAME" "/root/"
  log.info "uploaded env files to both local and remote."

  ssh.rsync_up.remote "$launch_dir/$vSTATIC_ENTRY_SH" "/root/"
  ssh.cmd.remote "chmod +x /root/$vSTATIC_ENTRY_SH"
  log.info "uploaded and set permissions for remote bootstrap script."
  #
  # Create the folder where we'll store out caprover
  # deployment tar files.
  #
  ssh.cmd.remote "mkdir -p /root/deployments"
  log.info "created remote deployment dir: /root/deployments"
  #
  # Before bootstrapping can occur, make sure we upload the .solos config folder
  # from our local machine to the remote machine.
  # Important: we don't need to do this with the docker container because we rely on mounting
  # the local .solos folder to the container.
  #
  if ssh.cmd.remote '[ -d '"${vSTATIC_DEBIAN_CONFIG_ROOT}"' ]'; then
    log.warn "remote already has a .solos config folder. skipping."
    log.info "see \`solos --help\` for how to re-sync your local or docker dev config folder to the remote."
  else
    ssh.cmd.remote "mkdir -p ${vSTATIC_DEBIAN_CONFIG_ROOT}"
    log.info "created empty remote .solos config folder."
    ssh.rsync_up.remote "$vSTATIC_MY_CONFIG_ROOT/" "$vSTATIC_DEBIAN_CONFIG_ROOT/"
    log.info "uploaded local .solos config folder to remote."
  fi
  #
  #
  # Note: the bootstrapping script should be idempotent but it's still
  # a long ass script so I like the ability to skip it. Also, the downsides of not re-running
  # are mitigated by the fact that every script that contributes to the bootstrapping
  # is available via an aliased command.
  #
  ssh.cmd.remote "/root/$vSTATIC_ENTRY_SH remote $vCLI_OPT_SERVER"
  #
  # I'm not doing anything with this status for now.
  # because I prefer that the bootstrapping script take
  # responsibility for idempotency and performance.
  #
  status.set "$vSTATUS_BOOTSTRAPPED_REMOTE" "$(utils.date)"
  log.info "bootstrapped the remote server."
  #
  # Bootstrapping postgres is a manual process so that's why I skip if
  # it was already created.
  #
  timestamp_of_postgres_bootstrap="$(status.get "$vSTATUS_BOOTSTRAPPED_POSTGRES")"
  if [ -n "$timestamp_of_postgres_bootstrap" ]; then
    log.warn "skipping all postgres bootstrap - previously completed at $timestamp_of_postgres_bootstrap"
  else
    ssh.rsync_down.remote "$vSTATIC_DEBIAN_CLONE_DIR/$vSTATIC_DB_ONE_CLICK_TEMPLATE_FILENAME" "$vCLI_OPT_DIR/"
    log.info "downloaded caprover one-click-app template."
    utils.echo_line
    echo ""
    cat "$vCLI_OPT_DIR/$vSTATIC_DB_ONE_CLICK_TEMPLATE_FILENAME"
    echo ""
    utils.echo_line
    echo "Setup the caprover Postgres database and hit enter."
    read -r
    status.set "$vSTATUS_BOOTSTRAPPED_POSTGRES" "$(utils.date)"
  fi
  #
  # Make sure the local bin executable is up to date.
  #
  ssh.ssh.rsync_up.docker "$vSTATIC_USR_LOCAL_BIN_EXECUTABLE" "/usr/local/bin/"
  #
  # The logic here is simpler because the bootstrap script for the docker container
  # will never deal with things like databases or service orchestration.
  #
  ssh.cmd.docker "$STORE_TARGET_NAMED_BOOTSTRAP_SH_FILE docker"
  status.set "$vSTATUS_BOOTSTRAPPED_DOCKER" "$(utils.date)"
  log.info "bootstrapped the local docker container."
  #
  # This is redundant, but it's a good safety check because
  # if something bad happened and the old ip is the same as the current
  # we'll end up destroying the current instance. Yikes.
  #
  local ip_to_deprovision="$(cache.get "ip_to_deprovision")"
  if [ "$ip_to_deprovision" == "$vENV_IP" ]; then
    log.error "Unexpected error: the ip to deprovision is the same as the current ip. Exiting."
    exit 1
  fi
  #
  # The active ip should never be empty.
  #
  if [ -z "$vENV_IP" ]; then
    log.error "Unexpected error: the current ip is empty. Exiting."
    exit 1
  fi
  if [ -n "$ip_to_deprovision" ]; then
    log.warn "waiting 5 seconds before destroying instance: $ip_to_deprovision"
    sleep 5
    vultr.compute.get_instance_id_from_ip "$ip_to_deprovision"
    local instance_id_to_deprovision="${vPREV_RETURN[0]}"
    if [ "$instance_id_to_deprovision" == "null" ]; then
      log.error "Unexpected error: couldn't find instance for ip: $ip_to_deprovision. Nothing to deprovision."
      exit 1
    fi
    vultr.compute.destroy_instance "$instance_id_to_deprovision"
    log.info "destroyed the previous instance with ip: $ip_to_deprovision"
  fi
  status.set "$vSTATUS_LAUNCH_SUCCEEDED" "$(utils.date)"
  log.success "launch completed successfully."
}
cmd.sync_config() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  if ! ssh.cmd.remote '[ -d '"${vSTATIC_DEBIAN_CONFIG_ROOT}"' ]'; then
    log.error "remote does not have a config folder. are you sure this server is launched? Exiting."
  fi
  log.warn "will overwrite the remote config folder in 5 seconds."
  sleep 3
  log.warn "will overwrite the remote config folder in 2 seconds."
  sleep 2
  log.warn "will overwrite the remote config folder in 1 second."
  sleep 1
  #
  # Do not rm -rf in the first command because we want as little downtime as possible.
  # instead, forcefully move the rsync'd local config folder to overwrite the remote one.
  #
  local tmp_dir="/root/.tmp"
  local config_dirname=".solos"
  ssh.cmd.remote "mkdir -p  ${tmp_dir}"
  log.info "remote tmp folder exists"
  ssh.cmd.remote "mkdir -p ${tmp_dir}/${config_dirname} && rm -rf ${tmp_dir}/${config_dirname}"
  log.info "wiped remote tmp/${config_dirname} folder in preparation for rsync."
  ssh.rsync_up.remote "$vSTATIC_MY_CONFIG_ROOT/" "${tmp_dir}/${config_dirname}/"
  log.info "uploaded local ${config_dirname} config folder to remote."
  ssh.cmd.remote "mv --force ${tmp_dir}/${config_dirname} ${vSTATIC_DEBIAN_CONFIG_ROOT}"
  log.info "overwrote remote ${config_dirname} config folder with our local one."
  ssh.cmd.remote "rm -rf ${tmp_dir}"
  log.info "cleaned up remote tmp folder."

  log.info "success: uploaded local .solos config folder to remote."
}
cmd.sync_bin() {
  cmd.checkout

  if [ "$vSTATIC_HOST" != "local" ]; then
    log.error "this command must be run from the local host. Exiting."
    exit 1
  fi
  precheck.docker_host_running
  ssh.ssh.rsync_up.docker "$vSTATIC_USR_LOCAL_BIN_EXECUTABLE" "/usr/local/bin/"
}
cmd.sync_env() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "remote" ]; then
    log.error "this command cannot be run from the remote host. Exiting."
    exit 1
  fi
  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  log.warn "will overwrite the remote .solos config folder in 5 seconds."
  sleep 3
  log.warn "will overwrite the remote .solos config folder in 2 seconds."
  sleep 2
  log.warn "will overwrite the remote .solos config folder in 1 second."
  sleep 1
  #
  # Do not rm -rf in the first command because we want as little downtime as possible.
  # instead, forcefully move the rsync'd local config folder to overwrite the remote one.
  #
  local tmp_dir="/root/.tmp"
  local local_env=$vCLI_OPT_DIR/$vSTATIC_ENV_FILENAME
  local local_env_sh=$vCLI_OPT_DIR/$vSTATIC_ENV_SH_FILENAME
  local tmp_env="$tmp_dir/$vSTATIC_ENV_SH_FILENAME"
  local tmp_env_sh="$tmp_dir/$vSTATIC_ENV_FILENAME"
  local target_env="/root/$vSTATIC_ENV_FILENAME"
  local target_env_sh="/root/$vSTATIC_ENV_SH_FILENAME"
  ssh.cmd.remote "mkdir -p  $tmp_dir"
  log.info "remote tmp folder exists"
  ssh.cmd.remote "rm -f ${tmp_env} && rm -f ${tmp_env_sh}"
  log.info "cleaned remote tmp dir in preparation for rsync."
  ssh.rsync_up.remote "$local_env" "$tmp_dir/"
  log.info "uploaded .env file to remote tmp folder."
  ssh.rsync_up.remote "$local_env_sh" "$tmp_dir/"
  log.info "uploaded .env.sh file to remote tmp folder."
  ssh.cmd.remote "mv --force $tmp_env_sh $target_env_sh && mv --force $tmp_env $target_env"
  log.info "overwrote remote .solos config folder with our local one."
  ssh.cmd.remote "rm -rf $tmp_dir"
  log.info "cleaned up remote tmp folder."

  log.info "success: uploaded local .env and .env.sh files to remote."
}
cmd.code() {
  cmd.checkout

  if ! command -v "code" &>/dev/null; then
    log.error "vscode is not installed to your path. cannot continue."
  fi
  if [ "$vSTATIC_HOST" != "local" ]; then
    log.error "this command must be run from the local host. Exiting."
    exit 1
  fi
  precheck.docker_host_running

  log.warn "would open vscode"
}
cmd.restore() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  log.warn "TODO: implementation needed"
}
cmd.backup() {
  cmd.checkout

  if [ "$vSTATIC_HOST" == "local" ]; then
    precheck.docker_host_running
  fi
  log.warn "TODO: implementation needed"
}
cmd.status() {
  local status_solos_id=""
  local status_container_runninng="NO"
  local status_launched="NO"
  local status_docker_ready="NO"
  local status_remote_ready="NO"
  local status_postgres_ready="NO"
  if [ -z "$vCLI_OPT_DIR" ]; then
    vCLI_OPT_DIR="$(cache.get "checkout_dir")"
  fi
  if [ -z "$vCLI_OPT_DIR" ]; then
    log.error "no project found at: $vCLI_OPT_DIR. if you moved it, use the --dir flag to specify the new location."
    exit 1
  fi
  if [ ! -f "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME" ]; then
    log.error "no project id found at: $vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME. this should never happen."
    exit 1
  fi
  #
  # If we're here, we know we have a project directory.
  # Let's go ahead and throw errors when base assumptions
  # like having a config dir are not met.
  #
  status_solos_id="$(cat "$vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME")"
  if [ -z "$status_solos_id" ]; then
    log.error "no solos id found at: $vCLI_OPT_DIR/$vSTATIC_SOLOS_ID_FILENAME. Exiting."
    exit 1
  fi
  if [ ! -d "$vSTATIC_MY_CONFIG_ROOT" ]; then
    log.error "no config dir found at: $vSTATIC_MY_CONFIG_ROOT. Exiting."
    exit 1
  fi
  #
  # Collect the status of the project.
  #
  if [ -n "$(status.get "$vSTATUS_LAUNCH_SUCCEEDED")" ]; then
    status_launched="YES"
  fi
  if [ -n "$(status.get "$vSTATUS_BOOTSTRAPPED_REMOTE")" ]; then
    status_remote_ready="YES"
  fi
  if [ -n "$(status.get "$vSTATUS_BOOTSTRAPPED_POSTGRES")" ]; then
    status_postgres_ready="YES"
  fi
  if [ -n "$(status.get "$vSTATUS_BOOTSTRAPPED_DOCKER")" ]; then
    status_docker_ready="YES"
  fi
  if precheck.docker_host_running &>/dev/null; then
    status_container_runninng="YES"
  fi
  log.info "CONFIG_DIR: $status_config_dir"
  log.info "DIR: $vCLI_OPT_DIR"
  log.info "STATUS: docker running: $status_container_runninng"
  log.info "PROJECT_ID: $status_solos_id"
  log.info "STATUS: fully launched: $status_launched"
  log.info "STATUS: bootstrapped remote: $status_remote_ready"
  log.info "STATUS: bootstrapped postgres: $status_postgres_ready"
  log.info "STATUS: bootstrapped docker: $status_docker_ready"
}
cmd.test() {
  cmd.checkout

  if [ "$vSTATIC_RUNNING_IN_GIT_REPO" == "true" ] && [ "$vSTATIC_HOST" == "local" ]; then
    test.variables
    test.bootfiles
  else
    log.error "this command can only be run from within a git repo."
    exit 1
  fi
}
# --------------------------------------------------------------------------------------------
#
# ENTRY
#
# parse the cli args and validate them.
#
flags.parse_requirements
flags.parse_cmd "$@"
flags.validate_options
#
# We seperate the parsing and mapping concerns so that
# frequent changes to business logic don't affect the parsing logic
#
solos.map_parsed_cli
if [ -z "$vCLI_OPT_DIR" ]; then
  log.error "--dir is required. Exiting."
  exit 1
fi
#
# Before doing ANYTHING, check that our command actually works.
# Fail fast!
#
if ! command -v "cmd.$vCLI_PARSED_CMD" &>/dev/null; then
  log.error "cmd.$vCLI_PARSED_CMD is not defined. Exiting."
  exit 1
fi
#
# We're only allowed to clear cache files associated with a particular named installation.
#
if [ "$vCLI_OPT_CLEAR_CACHE" == "true" ]; then
  state.clear.cache
fi
#
# Run the command specified in the cli args.
#
"cmd.$vCLI_PARSED_CMD"
